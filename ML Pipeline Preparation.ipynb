{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\josep\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\josep\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\josep\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\josep\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, make_scorer, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('sqlite:///data/DisasterResponse.db')\n",
    "df = pd.read_sql_table('DisasterResponse', con=engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.loc[:, 'message']\n",
    "y = df.iloc[:, 4:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization function to process text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    OUTPUT:\n",
    "    \"\"\"\n",
    "    # Normalize text\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "    # Split text into words using NLTK\n",
    "    words = word_tokenize(text)\n",
    "    # Remove stop words\n",
    "    words = [w for w in words if w not in stopwords.words(\"english\")]\n",
    "    # Reduce words to their root form\n",
    "    lemmed = [WordNetLemmatizer().lemmatize(w) for w in words]\n",
    "    # Lemmatize verbs by specifying pos\n",
    "    lemmed = [WordNetLemmatizer().lemmatize(w, pos='v').strip() for w in lemmed]\n",
    "    # Reduce words to their stems\n",
    "    stemmed = [PorterStemmer().stem(w) for w in lemmed]\n",
    "    tokenized = stemmed\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a machine learning pipeline\n",
    "This machine pipeline takes in the `message` column as input and output classification results on the other 36 categories in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect',\n",
       "                 CountVectorizer(tokenizer=<function tokenize at 0x00000197A83578B0>)),\n",
       "                ('tfidf', TfidfTransformer()),\n",
       "                ('clf',\n",
       "                 MultiOutputClassifier(estimator=RandomForestClassifier()))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model\n",
    "Reporting the f1 score, precision and recall for each output category of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = random_forest_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.84      0.95      0.89      5026\n",
      "               request       0.83      0.49      0.62      1184\n",
      "                 offer       0.00      0.00      0.00        32\n",
      "           aid_related       0.77      0.68      0.73      2819\n",
      "          medical_help       0.67      0.08      0.13       533\n",
      "      medical_products       0.77      0.07      0.14       324\n",
      "     search_and_rescue       0.67      0.03      0.06       180\n",
      "              security       0.33      0.01      0.02       122\n",
      "              military       0.77      0.08      0.14       221\n",
      "           child_alone       0.00      0.00      0.00         0\n",
      "                 water       0.91      0.35      0.50       465\n",
      "                  food       0.87      0.63      0.73       770\n",
      "               shelter       0.88      0.36      0.51       566\n",
      "              clothing       0.75      0.09      0.16        99\n",
      "                 money       0.80      0.02      0.05       170\n",
      "        missing_people       1.00      0.01      0.02        86\n",
      "              refugees       0.53      0.04      0.07       211\n",
      "                 death       0.87      0.16      0.26       309\n",
      "             other_aid       0.51      0.03      0.05       890\n",
      "infrastructure_related       0.00      0.00      0.00       413\n",
      "             transport       0.70      0.06      0.11       316\n",
      "             buildings       0.88      0.10      0.18       361\n",
      "           electricity       1.00      0.03      0.06       128\n",
      "                 tools       0.00      0.00      0.00        43\n",
      "             hospitals       0.00      0.00      0.00        72\n",
      "                 shops       0.00      0.00      0.00        23\n",
      "           aid_centers       0.00      0.00      0.00        75\n",
      "  other_infrastructure       1.00      0.00      0.01       290\n",
      "       weather_related       0.85      0.72      0.78      1762\n",
      "                floods       0.92      0.47      0.62       549\n",
      "                 storm       0.77      0.53      0.63       578\n",
      "                  fire       1.00      0.03      0.06        68\n",
      "            earthquake       0.89      0.81      0.85       577\n",
      "                  cold       0.89      0.06      0.11       138\n",
      "         other_weather       0.65      0.04      0.07       343\n",
      "         direct_report       0.82      0.37      0.51      1308\n",
      "\n",
      "             micro avg       0.83      0.53      0.65     21051\n",
      "             macro avg       0.64      0.20      0.25     21051\n",
      "          weighted avg       0.78      0.53      0.57     21051\n",
      "           samples avg       0.67      0.48      0.51     21051\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, zero_division=0, target_names=y.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying other machine learning algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(DecisionTreeClassifier()))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect',\n",
       "                 CountVectorizer(tokenizer=<function tokenize at 0x00000197A83578B0>)),\n",
       "                ('tfidf', TfidfTransformer()),\n",
       "                ('clf',\n",
       "                 MultiOutputClassifier(estimator=DecisionTreeClassifier()))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decision_tree_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = decision_tree_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reporting the f1 score, precision and recall for each output category of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.85      0.85      0.85      5026\n",
      "               request       0.59      0.55      0.57      1184\n",
      "                 offer       0.00      0.00      0.00        32\n",
      "           aid_related       0.67      0.63      0.65      2819\n",
      "          medical_help       0.37      0.32      0.34       533\n",
      "      medical_products       0.40      0.41      0.41       324\n",
      "     search_and_rescue       0.31      0.24      0.27       180\n",
      "              security       0.06      0.05      0.06       122\n",
      "              military       0.40      0.40      0.40       221\n",
      "           child_alone       0.00      0.00      0.00         0\n",
      "                 water       0.68      0.62      0.65       465\n",
      "                  food       0.73      0.72      0.73       770\n",
      "               shelter       0.63      0.62      0.62       566\n",
      "              clothing       0.55      0.53      0.54        99\n",
      "                 money       0.43      0.29      0.35       170\n",
      "        missing_people       0.17      0.13      0.15        86\n",
      "              refugees       0.29      0.27      0.28       211\n",
      "                 death       0.58      0.58      0.58       309\n",
      "             other_aid       0.31      0.29      0.30       890\n",
      "infrastructure_related       0.18      0.16      0.17       413\n",
      "             transport       0.29      0.30      0.30       316\n",
      "             buildings       0.47      0.39      0.42       361\n",
      "           electricity       0.42      0.40      0.41       128\n",
      "                 tools       0.12      0.09      0.10        43\n",
      "             hospitals       0.22      0.21      0.21        72\n",
      "                 shops       0.04      0.04      0.04        23\n",
      "           aid_centers       0.11      0.09      0.10        75\n",
      "  other_infrastructure       0.18      0.15      0.16       290\n",
      "       weather_related       0.71      0.73      0.72      1762\n",
      "                floods       0.65      0.57      0.61       549\n",
      "                 storm       0.65      0.66      0.65       578\n",
      "                  fire       0.31      0.37      0.34        68\n",
      "            earthquake       0.78      0.79      0.78       577\n",
      "                  cold       0.52      0.43      0.47       138\n",
      "         other_weather       0.28      0.26      0.27       343\n",
      "         direct_report       0.50      0.48      0.49      1308\n",
      "\n",
      "             micro avg       0.62      0.60      0.61     21051\n",
      "             macro avg       0.40      0.38      0.39     21051\n",
      "          weighted avg       0.62      0.60      0.61     21051\n",
      "           samples avg       0.51      0.49      0.46     21051\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, zero_division=0, target_names=y.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "kn_pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(KNeighborsClassifier()))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect',\n",
       "                 CountVectorizer(tokenizer=<function tokenize at 0x00000197A83578B0>)),\n",
       "                ('tfidf', TfidfTransformer()),\n",
       "                ('clf',\n",
       "                 MultiOutputClassifier(estimator=KNeighborsClassifier()))])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kn_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = kn_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reporting the f1 score, precision and recall for each output category of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.83      0.93      0.88      5026\n",
      "               request       0.75      0.45      0.56      1184\n",
      "                 offer       0.00      0.00      0.00        32\n",
      "           aid_related       0.71      0.42      0.53      2819\n",
      "          medical_help       0.63      0.09      0.15       533\n",
      "      medical_products       0.62      0.09      0.15       324\n",
      "     search_and_rescue       0.90      0.05      0.09       180\n",
      "              security       0.00      0.00      0.00       122\n",
      "              military       0.59      0.09      0.15       221\n",
      "           child_alone       0.00      0.00      0.00         0\n",
      "                 water       0.77      0.20      0.31       465\n",
      "                  food       0.74      0.33      0.45       770\n",
      "               shelter       0.73      0.19      0.30       566\n",
      "              clothing       0.70      0.16      0.26        99\n",
      "                 money       0.60      0.05      0.10       170\n",
      "        missing_people       1.00      0.01      0.02        86\n",
      "              refugees       0.50      0.04      0.07       211\n",
      "                 death       0.71      0.12      0.20       309\n",
      "             other_aid       0.37      0.05      0.09       890\n",
      "infrastructure_related       0.31      0.01      0.02       413\n",
      "             transport       0.85      0.07      0.13       316\n",
      "             buildings       0.67      0.08      0.15       361\n",
      "           electricity       0.76      0.10      0.18       128\n",
      "                 tools       0.00      0.00      0.00        43\n",
      "             hospitals       0.00      0.00      0.00        72\n",
      "                 shops       0.00      0.00      0.00        23\n",
      "           aid_centers       1.00      0.01      0.03        75\n",
      "  other_infrastructure       0.12      0.00      0.01       290\n",
      "       weather_related       0.74      0.42      0.53      1762\n",
      "                floods       0.73      0.17      0.28       549\n",
      "                 storm       0.74      0.27      0.39       578\n",
      "                  fire       0.50      0.04      0.08        68\n",
      "            earthquake       0.78      0.46      0.58       577\n",
      "                  cold       0.56      0.04      0.07       138\n",
      "         other_weather       0.59      0.03      0.06       343\n",
      "         direct_report       0.68      0.34      0.46      1308\n",
      "\n",
      "             micro avg       0.77      0.42      0.54     21051\n",
      "             macro avg       0.56      0.15      0.20     21051\n",
      "          weighted avg       0.70      0.42      0.48     21051\n",
      "           samples avg       0.65      0.41      0.45     21051\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, zero_division=0, target_names=y.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this dataset is imbalanced (ie some labels like water have few examples). I'm going to choose the machine learning algorithm that performs best at the f1_macro score, because some labels that have few frequency can actually be very important, like water. And with the f1_macro score we also take into account precision and recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the model further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying adding other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StartingVerbExtractor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def starting_verb(self, text):\n",
    "        # tokenize by sentences\n",
    "        sentence_list = nltk.sent_tokenize(text)\n",
    "        for sentence in sentence_list:\n",
    "            # tokenize each sentence into words and tag part of speech\n",
    "            pos_tags = nltk.pos_tag(tokenize(sentence))\n",
    "            if len(pos_tags) > 1:\n",
    "                # index pos_tags to get the first word and part of speech tag\n",
    "                first_word, first_tag = pos_tags[0]\n",
    "                # return true if the first word is an appropriate verb or RT for retweet\n",
    "                if first_tag in ['VB', 'VBP'] or first_word == 'RT':\n",
    "                    return 1\n",
    "        return 0\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # apply starting_verb function to all values in X\n",
    "        X_tagged = pd.Series(X).apply(self.starting_verb)\n",
    "        return pd.DataFrame(X_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextLengthExtractor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def textlength(self, text):\n",
    "        text_len = len(text.strip())\n",
    "        return text_len\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_tagged = pd.Series(X).apply(self.textlength)\n",
    "        return pd.DataFrame(X_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "\n",
    "        ('text_pipeline', Pipeline([\n",
    "            ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "            ('tfidf', TfidfTransformer())\n",
    "        ])),\n",
    "        \n",
    "        ('txt_length', TextLengthExtractor()),\n",
    "        ('starting_verb', StartingVerbExtractor())\n",
    "    ])),\n",
    "\n",
    "    ('clf', MultiOutputClassifier(DecisionTreeClassifier()))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('features',\n",
       "                 FeatureUnion(transformer_list=[('text_pipeline',\n",
       "                                                 Pipeline(steps=[('vect',\n",
       "                                                                  CountVectorizer(tokenizer=<function tokenize at 0x00000197A83578B0>)),\n",
       "                                                                 ('tfidf',\n",
       "                                                                  TfidfTransformer())])),\n",
       "                                                ('txt_length',\n",
       "                                                 TextLengthExtractor()),\n",
       "                                                ('starting_verb',\n",
       "                                                 StartingVerbExtractor())])),\n",
       "                ('clf',\n",
       "                 MultiOutputClassifier(estimator=DecisionTreeClassifier()))])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reporting the f1 score, precision and recall for each output category of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.84      0.84      0.84      5026\n",
      "               request       0.59      0.55      0.57      1184\n",
      "                 offer       0.00      0.00      0.00        32\n",
      "           aid_related       0.66      0.64      0.65      2819\n",
      "          medical_help       0.35      0.32      0.33       533\n",
      "      medical_products       0.40      0.41      0.41       324\n",
      "     search_and_rescue       0.29      0.25      0.27       180\n",
      "              security       0.08      0.07      0.07       122\n",
      "              military       0.42      0.43      0.43       221\n",
      "           child_alone       0.00      0.00      0.00         0\n",
      "                 water       0.66      0.60      0.63       465\n",
      "                  food       0.74      0.72      0.73       770\n",
      "               shelter       0.65      0.64      0.65       566\n",
      "              clothing       0.56      0.52      0.54        99\n",
      "                 money       0.39      0.25      0.31       170\n",
      "        missing_people       0.18      0.13      0.15        86\n",
      "              refugees       0.28      0.25      0.27       211\n",
      "                 death       0.57      0.57      0.57       309\n",
      "             other_aid       0.29      0.27      0.28       890\n",
      "infrastructure_related       0.18      0.18      0.18       413\n",
      "             transport       0.29      0.28      0.28       316\n",
      "             buildings       0.49      0.40      0.44       361\n",
      "           electricity       0.39      0.38      0.38       128\n",
      "                 tools       0.11      0.09      0.10        43\n",
      "             hospitals       0.20      0.18      0.19        72\n",
      "                 shops       0.00      0.00      0.00        23\n",
      "           aid_centers       0.11      0.08      0.09        75\n",
      "  other_infrastructure       0.15      0.12      0.14       290\n",
      "       weather_related       0.72      0.75      0.74      1762\n",
      "                floods       0.64      0.57      0.60       549\n",
      "                 storm       0.67      0.70      0.68       578\n",
      "                  fire       0.32      0.35      0.34        68\n",
      "            earthquake       0.78      0.79      0.79       577\n",
      "                  cold       0.49      0.41      0.45       138\n",
      "         other_weather       0.27      0.25      0.26       343\n",
      "         direct_report       0.51      0.47      0.49      1308\n",
      "\n",
      "             micro avg       0.62      0.60      0.61     21051\n",
      "             macro avg       0.40      0.37      0.38     21051\n",
      "          weighted avg       0.61      0.60      0.61     21051\n",
      "           samples avg       0.52      0.50      0.46     21051\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, zero_division=0, target_names=y.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding these 2 features actually decreased the f1_macro score, I'm going to try removing the Starting with Verb Feature, since this feature is not very robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "\n",
    "        ('text_pipeline', Pipeline([\n",
    "            ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "            ('tfidf', TfidfTransformer())\n",
    "        ])),\n",
    "        \n",
    "        ('txt_length', TextLengthExtractor())\n",
    "    ])),\n",
    "\n",
    "    ('clf', MultiOutputClassifier(DecisionTreeClassifier()))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('features',\n",
       "                 FeatureUnion(transformer_list=[('text_pipeline',\n",
       "                                                 Pipeline(steps=[('vect',\n",
       "                                                                  CountVectorizer(tokenizer=<function tokenize at 0x00000197A83578B0>)),\n",
       "                                                                 ('tfidf',\n",
       "                                                                  TfidfTransformer())])),\n",
       "                                                ('txt_length',\n",
       "                                                 TextLengthExtractor())])),\n",
       "                ('clf',\n",
       "                 MultiOutputClassifier(estimator=DecisionTreeClassifier()))])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reporting the f1 score, precision and recall for each output category of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.84      0.83      0.84      5026\n",
      "               request       0.58      0.55      0.56      1184\n",
      "                 offer       0.00      0.00      0.00        32\n",
      "           aid_related       0.66      0.64      0.65      2819\n",
      "          medical_help       0.37      0.34      0.35       533\n",
      "      medical_products       0.40      0.41      0.40       324\n",
      "     search_and_rescue       0.26      0.23      0.25       180\n",
      "              security       0.06      0.04      0.05       122\n",
      "              military       0.38      0.37      0.38       221\n",
      "           child_alone       0.00      0.00      0.00         0\n",
      "                 water       0.68      0.60      0.64       465\n",
      "                  food       0.75      0.71      0.73       770\n",
      "               shelter       0.63      0.61      0.62       566\n",
      "              clothing       0.56      0.55      0.55        99\n",
      "                 money       0.37      0.24      0.29       170\n",
      "        missing_people       0.19      0.13      0.15        86\n",
      "              refugees       0.26      0.24      0.25       211\n",
      "                 death       0.58      0.58      0.58       309\n",
      "             other_aid       0.28      0.26      0.27       890\n",
      "infrastructure_related       0.18      0.18      0.18       413\n",
      "             transport       0.30      0.28      0.29       316\n",
      "             buildings       0.49      0.39      0.44       361\n",
      "           electricity       0.38      0.37      0.37       128\n",
      "                 tools       0.15      0.12      0.13        43\n",
      "             hospitals       0.21      0.22      0.21        72\n",
      "                 shops       0.04      0.04      0.04        23\n",
      "           aid_centers       0.11      0.09      0.10        75\n",
      "  other_infrastructure       0.13      0.11      0.12       290\n",
      "       weather_related       0.73      0.75      0.74      1762\n",
      "                floods       0.64      0.57      0.60       549\n",
      "                 storm       0.68      0.70      0.69       578\n",
      "                  fire       0.28      0.34      0.30        68\n",
      "            earthquake       0.78      0.80      0.79       577\n",
      "                  cold       0.50      0.43      0.46       138\n",
      "         other_weather       0.29      0.26      0.28       343\n",
      "         direct_report       0.48      0.45      0.47      1308\n",
      "\n",
      "             micro avg       0.62      0.59      0.61     21051\n",
      "             macro avg       0.39      0.37      0.38     21051\n",
      "          weighted avg       0.61      0.59      0.60     21051\n",
      "           samples avg       0.51      0.49      0.46     21051\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, zero_division=0, target_names=y.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still the Original pipeline performed better than with these 2 features added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tuning the model for accuracy, precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = {\"Accuracy\": \"accuracy\", \"Precision\": make_scorer(precision_score, average='macro', zero_division=0), \n",
    "           \"Recall\": make_scorer(recall_score, average='macro', zero_division=0), \n",
    "           \"F1\": make_scorer(f1_score, average='macro', zero_division=0)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using grid search to find better parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE THIS PARAMETERS AND GRIDSEARCHCV WHEN COMPUTATIONAL POWER AVAILABLE\n",
    "# parameters = {\n",
    "#     'vect__ngram_range': ((1, 1), (1, 2)),\n",
    "#     'vect__max_df': (0.5, 0.75, 1.0),\n",
    "#     'vect__max_features': (None, 5000, 10000),\n",
    "#     'tfidf__use_idf': (True, False),\n",
    "#     'clf__estimator__criterion': ['gini','entropy'],\n",
    "#     'clf__estimator__min_samples_split': [2, 3, 4],\n",
    "# }\n",
    "#\n",
    "# decision_tree_cv = GridSearchCV(random_forest_pipeline, param_grid=parameters, return_train_score=True, \n",
    "#                         refit='F1', scoring = scoring)\n",
    "\n",
    "parameters = {\n",
    "    'clf__estimator__criterion': ['gini','entropy']\n",
    "}\n",
    "\n",
    "decision_tree_cv = GridSearchCV(decision_tree_pipeline, param_grid=parameters, verbose=3, cv=2, return_train_score=True, \n",
    "                        refit='F1', scoring = scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n",
      "[CV 1/2] END clf__estimator__criterion=gini; Accuracy: (train=0.997, test=0.174) F1: (train=0.971, test=0.366) Precision: (train=0.972, test=0.386) Recall: (train=0.969, test=0.352) total time=11.3min\n",
      "[CV 2/2] END clf__estimator__criterion=gini; Accuracy: (train=0.997, test=0.166) F1: (train=0.971, test=0.370) Precision: (train=0.972, test=0.382) Recall: (train=0.971, test=0.359) total time=12.0min\n",
      "[CV 1/2] END clf__estimator__criterion=entropy; Accuracy: (train=0.997, test=0.175) F1: (train=0.971, test=0.372) Precision: (train=0.972, test=0.394) Recall: (train=0.969, test=0.355) total time=10.2min\n",
      "[CV 2/2] END clf__estimator__criterion=entropy; Accuracy: (train=0.997, test=0.170) F1: (train=0.971, test=0.371) Precision: (train=0.972, test=0.391) Recall: (train=0.971, test=0.355) total time= 9.4min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2,\n",
       "             estimator=Pipeline(steps=[('vect',\n",
       "                                        CountVectorizer(tokenizer=<function tokenize at 0x00000197A83578B0>)),\n",
       "                                       ('tfidf', TfidfTransformer()),\n",
       "                                       ('clf',\n",
       "                                        MultiOutputClassifier(estimator=DecisionTreeClassifier()))]),\n",
       "             param_grid={'clf__estimator__criterion': ['gini', 'entropy']},\n",
       "             refit='F1', return_train_score=True,\n",
       "             scoring={'Accuracy': 'accuracy',\n",
       "                      'F1': make_scorer(f1_score, average=macro, zero_division=0),\n",
       "                      'Precision': make_scorer(precision_score, average=macro, zero_division=0),\n",
       "                      'Recall': make_scorer(recall_score, average=macro, zero_division=0)},\n",
       "             verbose=3)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decision_tree_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Showing the accuracy, precision, and recall of the models.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Params</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'clf__estimator__criterion': 'gini'}</td>\n",
       "      <td>0.170022</td>\n",
       "      <td>0.383888</td>\n",
       "      <td>0.355569</td>\n",
       "      <td>0.367836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'clf__estimator__criterion': 'entropy'}</td>\n",
       "      <td>0.172378</td>\n",
       "      <td>0.392460</td>\n",
       "      <td>0.355098</td>\n",
       "      <td>0.371523</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Params  Accuracy  Precision    Recall  \\\n",
       "0     {'clf__estimator__criterion': 'gini'}  0.170022   0.383888  0.355569   \n",
       "1  {'clf__estimator__criterion': 'entropy'}  0.172378   0.392460  0.355098   \n",
       "\n",
       "         F1  \n",
       "0  0.367836  \n",
       "1  0.371523  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = []\n",
    "cvres = decision_tree_cv.cv_results_\n",
    "for mean_test_Accuracy, mean_test_Precision, mean_test_Recall, mean_test_F1, params in zip(cvres['params'], \n",
    "                                                                                           cvres['mean_test_Accuracy'], \n",
    "                                                                                           cvres['mean_test_Precision'], \n",
    "                                                                                           cvres['mean_test_Recall'], \n",
    "                                                                                           cvres['mean_test_F1']):\n",
    "    rows.append([mean_test_Accuracy, mean_test_Precision, mean_test_Recall, mean_test_F1, params])\n",
    "scores = pd.DataFrame(rows, columns=[\"Params\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\"])\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2,\n",
       "             estimator=Pipeline(steps=[('vect',\n",
       "                                        CountVectorizer(tokenizer=<function tokenize at 0x00000197A83578B0>)),\n",
       "                                       ('tfidf', TfidfTransformer()),\n",
       "                                       ('clf',\n",
       "                                        MultiOutputClassifier(estimator=DecisionTreeClassifier()))]),\n",
       "             param_grid={'clf__estimator__criterion': ['gini', 'entropy']},\n",
       "             refit='F1', return_train_score=True,\n",
       "             scoring={'Accuracy': 'accuracy',\n",
       "                      'F1': make_scorer(f1_score, average=macro, zero_division=0),\n",
       "                      'Precision': make_scorer(precision_score, average=macro, zero_division=0),\n",
       "                      'Recall': make_scorer(recall_score, average=macro, zero_division=0)},\n",
       "             verbose=3)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decision_tree_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect',\n",
       "                 CountVectorizer(tokenizer=<function tokenize at 0x00000197A83578B0>)),\n",
       "                ('tfidf', TfidfTransformer()),\n",
       "                ('clf',\n",
       "                 MultiOutputClassifier(estimator=DecisionTreeClassifier(criterion='entropy')))])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_estimator = decision_tree_cv.best_estimator_\n",
    "best_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_estimator.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reporting the f1 score, precision and recall for each output category of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.85      0.86      0.86      5026\n",
      "               request       0.58      0.54      0.56      1184\n",
      "                 offer       0.00      0.00      0.00        32\n",
      "           aid_related       0.67      0.64      0.66      2819\n",
      "          medical_help       0.38      0.31      0.34       533\n",
      "      medical_products       0.46      0.40      0.43       324\n",
      "     search_and_rescue       0.26      0.19      0.22       180\n",
      "              security       0.12      0.08      0.10       122\n",
      "              military       0.43      0.38      0.40       221\n",
      "           child_alone       0.00      0.00      0.00         0\n",
      "                 water       0.69      0.60      0.64       465\n",
      "                  food       0.76      0.73      0.74       770\n",
      "               shelter       0.63      0.61      0.62       566\n",
      "              clothing       0.54      0.51      0.52        99\n",
      "                 money       0.37      0.29      0.33       170\n",
      "        missing_people       0.19      0.10      0.13        86\n",
      "              refugees       0.34      0.26      0.29       211\n",
      "                 death       0.60      0.59      0.60       309\n",
      "             other_aid       0.32      0.25      0.28       890\n",
      "infrastructure_related       0.19      0.16      0.18       413\n",
      "             transport       0.34      0.27      0.30       316\n",
      "             buildings       0.48      0.41      0.44       361\n",
      "           electricity       0.37      0.34      0.36       128\n",
      "                 tools       0.03      0.02      0.03        43\n",
      "             hospitals       0.18      0.15      0.17        72\n",
      "                 shops       0.00      0.00      0.00        23\n",
      "           aid_centers       0.12      0.08      0.10        75\n",
      "  other_infrastructure       0.18      0.13      0.15       290\n",
      "       weather_related       0.73      0.74      0.73      1762\n",
      "                floods       0.64      0.58      0.61       549\n",
      "                 storm       0.65      0.70      0.67       578\n",
      "                  fire       0.36      0.29      0.32        68\n",
      "            earthquake       0.79      0.79      0.79       577\n",
      "                  cold       0.56      0.42      0.48       138\n",
      "         other_weather       0.22      0.19      0.20       343\n",
      "         direct_report       0.53      0.46      0.49      1308\n",
      "\n",
      "             micro avg       0.64      0.60      0.62     21051\n",
      "             macro avg       0.40      0.36      0.38     21051\n",
      "          weighted avg       0.62      0.60      0.61     21051\n",
      "           samples avg       0.52      0.50      0.46     21051\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, zero_division=0, target_names=y.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting the model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/classifier.pkl']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(best_estimator, \"models/classifier.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
